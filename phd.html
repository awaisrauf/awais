<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
        <title>PhD Application - Séb Arnold</title>

        <!-- Bootstrap -->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/sandstone/bootstrap.min.css" />

        <!--Prism for code high-lighting-->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/themes/prism-solarizedlight.css" / >

        <!--KaTeX for fast embedded math-->
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css">

        <!--Pseudocode.js-->
        <link rel="stylesheet" href="https://cdn.rawgit.com/seba-1511/cdn/master/pseudocode.js/pseudocode.min.css">


        <style type="text/css" media="all">
        /* Space out content a bit */
        body {
            padding-top: 20px;
            padding-bottom: 20px;
        }

        p {
            font-size: 16px;
            text-align: justify;
        }

        /* Everything but the jumbotron gets side spacing for mobile first views */
        .header,
        .footer {
            padding-right: 15px;
            padding-left: 15px;
        }

        /* Custom page header */
        .header {
            padding-bottom: 20px;
            border-bottom: 1px solid #e5e5e5;
        }
        /* Make the masthead heading the same height as the navigation */
        .header h3 {
            margin-top: 0;
            margin-bottom: 0;
            line-height: 40px;
        }

        img {
            max-width: 100%;
        }

        /* Custom page footer */
        .footer {
            padding-top: 19px;
            color: #777;
            border-top: 1px solid #e5e5e5;
        }

        /* Customize container */
        @media (min-width: 768px) {
            .container {
                                max-width: 730px;
                            }
        }
        .container-narrow > hr {
            margin: 30px 0;
        }

        /* Responsive: Portrait tablets and up */
        @media screen and (min-width: 768px) {
            /* Remove the padding we set earlier */
            .header,
            .marketing,
            .footer {
                padding-right: 0;
                padding-left: 0;
            }
            /* Space out the masthead */
            .header {
                margin-bottom: 30px;
            }
            /* Remove the bottom border on the jumbotron for visual effect */
            .jumbotron {
                border-bottom: 0;
            }
        }

        .well {
            border: 1px solid #767676;
            width: 157px;
            max-width: 157px;
        }
        .well a {
            color:#767676;
            margin-bottom:5px;
        }
        .well ul {
            list-style: none;
            margin: 0px;
            padding-left: 10px;
        }
        </style>

        <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
            <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
        <![endif]-->


        <!--Plotly.js-->
        <!--Needs to be imported before body, else figs won't load.-->
        <script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>

    </head>
    <body>
        <div class="container">
            <div class="header clearfix">
                <!--<nav>-->
                <!--<ul class="nav nav-pills pull-right">-->
                <!--<li role="presentation" class="active"><a href="#">Home</a></li>-->
                <!--<li role="presentation"><a href="#">About</a></li>-->
                <!--<li role="presentation"><a href="#">Contact</a></li>-->
                <!--</ul>-->
                <!--</nav>-->

                                <h1 class="text-center">PhD Application</h1>
                <h4 class="text-sm text-muted text-center"> by Séb Arnold,  <span style="font-weight:normal;"><i>November 26, 2016</i></span></h4>
                            </div>

                        <div style="margin-top:20px;margin-bottom:20px;"><p><p style="text-align:center;margin:0px;"><b>Abstract</b><p><br/>
            I am currently applying to PhD programs that start in Autumn of 2017. This document provides a short overview of the different aspects of my undergraduate experience.
            
             </p></div>
            <h1 id="timeline">Timeline</h1>
            <iframe src="https://cdn.knightlab.com/libs/timeline3/latest/embed/index.html?source=1wHvwpC-LW7OcPFPNBU4eQpHBwxrxlMveI5yqOcbCEEo&amp;font=OpenSans-GentiumBook&amp;lang=en&amp;initial_zoom=2&amp;height=650" width="100%" height="650" frameborder="0">
            </iframe>
            <h1 id="very-recent-works">Very Recent Works</h1>
            <ul>
            <li><a href="https://seba-1511.github.io/randopt/">Randopt</a></li>
            <li><a href="https://seba-1511.github.io/mj_transfer/">MuJoCo Transfer Learning Environments</a></li>
            <li>Introduction to Distributed Deep Learning (<a href="https://seba-1511.github.io/dist_blog/">Online draft</a>)</li>
            </ul>
            <h1 id="research-experience">Research Experience</h1>
            <p>During my 3 years at USC, I was fortunate to work in a couple of research groups and under the supervision of several inspiring researchers. I will hopefully be able to include all of our results in my future bachelor thesis.</p>
            <h2 id="information-sciences-institute">Information Sciences Institute</h2>
            <p>In the 2015 Spring Semester, I collaborated with Dr. Greg Ver Steeg and Dr. Itay Hen at the Information Science Institute. In our first project, Dr. Hen and I decided to investigate the efficiency of a special form of quantum computers - namely quantum annealers. As a matter of fact, USC is one of only two institutions<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> to have access to a D-Wave annealer, which I saw as an excellent opportunity to understand quantum computers in a hands-on environment. In particular we wanted to replicate and understand results obtained from another research group who demonstrated better scaling on Ising optimization problems using simulated annealing. My task in this project was to implement a state-of-the-art version of simulated and quantum annealing, and figure out which problems the quantum annealer solved faster. It turns out that we were not the only ones interested in this problem, and the Google-NASA collaboration found ways to accelerate quantum annealing such that they reached a <span class="math inline">\(10^8\)</span> speedup<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> with respect to simulated annealing. I decided to leave the project after having efficiently implemented both algorithms and gained a working knowledge of quantum computing. My main motivation was that I was more and more attracted by problems in another field: deep learning. </p>
            <div class="figure">
            <img src="phd_figs/specialists.png" alt="An example of the generalist-specialist setup." />
            <p class="caption">An example of the generalist-specialist setup.</p>
            </div>
            <p>Throughout the fall of 2015, Dr. Ver Steeg allowed me to work on the project of my choice. At that time, I was fascinated by an idea I had seen in <span class="citation">[<a href="#ref-distill">1</a>]</span>, which consisted of using an ensemble of neural networks as a way to improve classification performance. This is the so called <em>generalist-specialist</em> framework, and is easily understood through the following analogy; when feeling ill, you first go to a general practitioner who might then send you to a specialist. By replacing doctors with neural networks, this can be directly applied to image classification where the generalist could predict the family of object in the image (eg, vehicles), and the specialists will refine the original prediction. (eg, tow truck) The problem I decided to tackle was to find a better class-assignment algorithm for the specialists, which was previously done using variations of the k-means clustering algorithms. The results were summarized in the paper &quot;A Greedy Algorithm to Cluster Specialists&quot; <span class="citation">[<a href="#ref-specialists">2</a>]</span>, which is my first publication.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> Despite obtaining mitigated results (my method only gets about 2% improvement), I am particularly proud that I was able to autonomously designed and executed the entire research project. (Including the management of more than 200Gb of produced data with my desktop GPU.)</p>
            <h2 id="modeling-and-simulation-lab">Modeling and Simulation Lab</h2>
            <p>My main takeaway from our project with Dr. Hen is that every problem can be phrased as an optimization problem. With a long-standing interest in distributed computing and my recent work at <a href="#nervana-internship">Nervana Systems</a>, it was only a matter of time before I merged both interests. In the Spring of 2016, I asked Prof. Chunming Wang if he would be interested in exploring distributed algorithms to train neural networks. He readily accepted, and we began benchmarking existing stochastic gradients algorithms in the sequential and distributed (synchronous and asynchronous) settings. </p>
            <div class="figure">
            <img src="phd_figs/sync_compare.png" alt="Benchmark of synchronous distributed algorithms on the Six-Humps toy problem." />
            <p class="caption">Benchmark of synchronous distributed algorithms on the Six-Humps toy problem.</p>
            </div>
            <p>Although the field of distributed optimization has existed for a long time, the loss surface of deep neural networks is poorly understood and a very active research topic. Experts tend to agree that in very high-dimensions, the number of local minimas decreases exponentially and for neural-networks those local minimas are &quot;not that bad&quot;. <span class="citation">[<a href="#ref-dauphin">3</a>, <a href="#ref-losssurface">4</a>]</span> Our approach is to gather as much information about the Hessian of the loss function as we can, by taking advantage of the asynchrony in the distributed setting. I am responsible for designing and implementing the experiments, while the theory - and mathematical rigorousity - is developed in our meetings with Prof. Wang.</p>
            <h2 id="brain-body-dynamics-lab">Brain Body Dynamics Lab</h2>
            <p>With an expanding understanding of the theory of optimization of neural networks and efficient distributed implementations, I was looking for an interesting project where I could apply those new skills. As a matter of fact, I believe the best researchers are able to produce both solid theoretical results - to impress their peers - as well as impressive applications - to impress their grandmothers. </p>
            <div class="figure">
            <img src="phd_figs/finger_screenshots.png" alt="A simulated tendon-driven finger." />
            <p class="caption">A simulated tendon-driven finger.</p>
            </div>
            <p>In the early summer of 2016, I was given a chance to tour Prof. Francisco Valero-Cuevas' lab. In particular I connected with Brian Cohn, and we started looking for opportunities to apply distributed deep reinforcement learning to problems related to optimal control. By the end of July, we decided to explore transfer learning from a simulation, to a robotic, and finally to a human finger. The goal is to use the neural network as a controller so that by the end of the training procedure it is able to control a cadaveric finger activated by its tendons. Our current strategy is to mix TRPO <span class="citation">[<a href="#ref-trpo">5</a>]</span> with progressive neural networks <span class="citation">[<a href="#ref-pnn">6</a>]</span>, using ideas from probabilistic differential dynamic programming <span class="citation">[<a href="#ref-pddp">7</a>]</span>. My task is to develop the mathematical and experimental framework behind the experiments, and our early results <span class="citation">[<a href="#ref-trpocem">8</a>]</span> with Prof. Valero-Cuevas have been accepted as a poster paper at the SoCal Machine Learning Symposium. For those experiments, I developed the fastest implementation of TRPO, and distributed it over GPUs. I am also designing custom environments for transfer learning in reinforcement setups, as well as a convenient random search library.</p>
            <h1 id="nervana-internship">Nervana Internship</h1>
            <p>During the summer of 2015, Nervana Systems asked me if I would be interested in joining their team as an intern for the fall semester. This request came after several of my contributions being merged in their framework. From August 2015 to August 2016, I was thus interning with them remote and part-time. This would not have been possible without the help of Dr. Michael Shindler who supervised me throughout the internship. </p>
            <div class="figure">
            <img src="phd_figs/gorila.png" alt="The Gorila setup for Distributed Deep Reinforcement Learning." />
            <p class="caption">The Gorila setup for Distributed Deep Reinforcement Learning.</p>
            </div>
            <p>While at Nervana I worked on two projects, mostly independent of the rest of the team. This allowed me to work at my own pace - a requirement since I was working part-time. The project I was tasked with was to replicate the work originated at DeepMind which was named the <em>General Reinforcement Learning Architecture</em> <span class="citation">[<a href="#ref-gorila">9</a>]</span>, or <em>Gorila</em>. Gorila is a distributed architecture and is summarized in four parts:</p>
            <ul>
            <li><span class="math inline">\(N\)</span> <strong>actors</strong> whose goal is to generate as much experience from the environment as possible. For example, in the game of chess this would mean playing as many games as possible using the current parameters. Hence, no learning is involved.</li>
            <li>A <strong>replay memory</strong> which stores and centralizes the experience accumulated by the actors. Think of it as a database of states, rewards, and actions.</li>
            <li>A <strong>learner</strong> who takes the experience from the memory replay and executes the optimization procedure. That's where parameters are updated.</li>
            <li>A <strong>manager</strong> responsible for the coordination of all other processes. This includes start-up and shutdown management, parameters distribution, etc...</li>
            </ul>
            <p>This implementation of Gorila is the only one I am aware of. (Besides the original one developed by DeepMind)</p>
            <div class="figure">
            <img src="phd_figs/pipeline.png" alt="Our ndist pipeline for distributed deep learning." />
            <p class="caption">Our ndist pipeline for distributed deep learning.</p>
            </div>
            <p>In my second project at Nervana, I extended the distributed training effort of the company. I investigated several approaches using MPI. In particular, we were interested in very large neural network optimization (with several dozens of millions of parameters) and concluded that tree-based reductions were our best option. While I worked on scaling the training procedure across machines using fast Infiniband connects, a colleague worked on improving the efficiency of intra-node communication. We were both implementing different versions of the all-reduce algorithm and were obtaining different results because of the network architecture of the computational system. I then suggested to merge our efforts into the <em>Nervana Distributed Pipeline</em> or <em>ndist</em>. Ndist essentially allows us to use both fast GPU kernels for intra-node communication, and MPI for inter-node communication. In addition, by clearly separating those two levels, we are now able to explore more interesting and complex training and reduction schemes. For example, we can use a ring-based reduction algorithm at the GPU level while using a tree-based one at the machine level. Similarly, we can use different optimization algorithms at each computational level which permits more flexibility with respect to asynchrony. We are currently in the midst of testing and writing the paper about ndist, but the recent acquisition<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> of Nervana by Intel has added a bit of paperwork overhead.</p>
            <h1 id="eth-zürich">ETH Zürich</h1>
            <p>It is no secret that before joining USC, I was a student at ETH Zürich. In the following section I'll shortly describe my work in the Computational Social Science Group.</p>
            <h2 id="computational-social-science">Computational Social Science</h2>
            <p>As a freshman, I joined the COMS of Prof. Dirk Helbing in the Spring of 2012. (In Europe, it is rather unusual for first-year students to join a research group.) I was under the supervision of Dr. Stefano Balietti, at the time a PhD student. Together we designed a social infrastructure for researchers to communicate about their work. That platform was part of a larger project spanning multiple universities in Europe, and we received funding and appraisal from the European Commission for the coming years. The entirety of this work was built on top of web technologies, and more information can be found on <a href="http://www.qlectives.eu">QLectives.eu</a>. </p>
            <p>While a student at the COMS, I also implemented part of the infrastructure for a mass experiment trying to quantify the meritocratic behaviour in groups of people. As far as I know, those results have never been published.</p>
            <h1 id="personal-projects">Personal Projects</h1>
            <p>I've been working on a few personal projects, and here are a few lines about some of them.</p>
            <h4 id="tooski">Tooski</h4>
            <p>As I was learning to program, I used to ski competitively. I merged both passions and founded <a href="http://www.tooski.ch">Tooski</a>, which is a website about ski racing. It is currently the most visited French-speaking skiing website, and we are particularly proud to support the Swiss youth through blogs and sponsoring. The lead of this project has now been handed over to a group of people based in Europe who can maintain the website and more closely follow the evolution of this promising Swiss youth.</p>
            <h4 id="paperify">Paperify</h4>
            <p>I've always liked to build my own tools. Paperify is a set of custom templates, wrappers, filters, and other small hacks that allow me to produce professional documents. This document, <a href="http://www.seba1511.com">my website</a>, my resume are all written in markdown - which forces the focus on the content rather than the form - and then converted to the appropriate format using paperify. Using that same workflow, it is possible to generate PDFs,  documents, presentations, and web pages.</p>
            <h4 id="languages">Languages</h4>
            <p>I was fortunate to be the offspring of an Italian mother and a Swiss father, in a country with four official languages. Given that my father is a translator, it is not surprising that I am generally interested in learning new languages as soon as I have a chance. Lately I've been taking a stab at Spanish and I hope to become as fluent in it as I am in French, Italian, German and English.</p>
            <h1 id="references">References</h1>
            <p>Some of the papers I referenced in this document.</p>
            <div id="refs" class="references">
            <div id="ref-distill">
            <p>1. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. (2015).</p>
            </div>
            <div id="ref-specialists">
            <p>2. Arnold, S.: A Greedy Algorithm to Cluster Specialists. ArXiv e-prints. (2016).</p>
            </div>
            <div id="ref-dauphin">
            <p>3. Dauphin, Y.N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., Bengio, Y.: Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In: Advances in neural information processing systems. pp. 2933–2941 (2014).</p>
            </div>
            <div id="ref-losssurface">
            <p>4. Choromanska, A., Henaff, M., Mathieu, M., Arous, G.B., LeCun, Y.: The loss surfaces of multilayer networks. Presented at the (2015).</p>
            </div>
            <div id="ref-trpo">
            <p>5. Schulman, J., Levine, S., Moritz, P., Jordan, M.I., Abbeel, P.: Trust region policy optimization.</p>
            </div>
            <div id="ref-pnn">
            <p>6. Rusu, A.A., Rabinowitz, N.C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., Hadsell, R.: Progressive neural networks. arXiv preprint arXiv:1606.04671. (2016).</p>
            </div>
            <div id="ref-pddp">
            <p>7. Pan, Y., Theodorou, E.: Probabilistic differential dynamic programming. In: Advances in neural information processing systems. pp. 1907–1915 (2014).</p>
            </div>
            <div id="ref-trpocem">
            <p>8. Arnold, S., Chu, E., Valero-Cuevas, F.: Performance comparison between trpo and cem.</p>
            </div>
            <div id="ref-gorila">
            <p>9. Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., De Maria, A., Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., others: Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296. (2015).</p>
            </div>
            </div>
            <div class="footnotes">
            <hr />
            <ol>
            <li id="fn1"><p>The other being a collaboration between Google and NASA.<a href="#fnref1">↩</a></p></li>
            <li id="fn2"><p>&quot;When can Quantum Annealing win ?&quot;, <a href="https://goo.gl/gQiWJ0" class="uri">https://goo.gl/gQiWJ0</a><a href="#fnref2">↩</a></p></li>
            <li id="fn3"><p>Not yet published, but available on the pre-print website <a href="http://www.arxiv.org">ArXiv</a>.<a href="#fnref3">↩</a></p></li>
            <li id="fn4"><p>Reportedly for more than $400M. Not bad for 45 full-timers.<a href="#fnref4">↩</a></p></li>
            </ol>
            </div>
            
            <footer class="footer">
                <p><b>PhD Application</b> - <i>Séb Arnold</i>, November 26, 2016.</p>
            </footer>

        </div> <!-- /container -->

        <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
        <!-- Include all compiled plugins (below), or include individual files as needed -->
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js"></script>

        <!--Prism for code highlighting-->
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/prism.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/components/prism-python.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/components/prism-c.min.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.5.1/components/prism-java.min.js"></script>

        <!--MathJax-->
        <script type="text/x-mathjax-config">
        var delim = '\u0024';
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [[delim, delim], ['\\(','\\)']]}
        });
        </script>
        <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

        <!--KaTeX JavaScript-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
        <!--<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>-->

        <!--Pseudocode.js-->
        <script src="https://cdn.rawgit.com/seba-1511/cdn/master/pseudocode.js/pseudocode.min.js"></script>
        <!--<script src="https://rawgit.com/seba-1511/cdn/master/pseudocode.js/pseudocode.min.js"></script>-->

        <!--Custom scripting-->
        <script type="text/javascript">
        // Allows prism to work properly
        jQuery(document).ready(function() {
            jQuery('.python').addClass('language-python').removeClass('python');
            jQuery('.javascript').addClass('language-js').removeClass('javascript');
            jQuery('.c').addClass('language-c').removeClass('c');
            jQuery('.java').addClass('language-java').removeClass('java');
            jQuery('.sourceCode').removeClass('sourceCode');
            jQuery('table').addClass('table table-striped table-bordered');
            jQuery('img').addClass('img-responsive');
            // renderMathInElement(document.body, {
            //     displayMode: false,
            //     throwOnError: false,
            //     errorColor: '#cc0000',
            // });

            var math = document.getElementsByClassName("math");
            // MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            MathJax.Hub.Queue([math, ]);
            Prism.highlightAll(false);

            // The following uses pseudocode.js to render algorithms
            var i, content, container;
            var pseudocodeElems = document.querySelectorAll('pre.algo code');
            var parents = document.querySelectorAll('pre.algo');
            var displayOptions = {
                indentSize: '1.5em',
                commentDelimiter: '//',
                lineNumber: true,
                lineNumberPunc: ':',
                noEnd: true,
                captionCount: 1,
                throwOnError: false,
            };
            for (i=0; i < pseudocodeElems.length; i++) {
                content = pseudocodeElems[i].textContent;
                container = document.createElement('div');
                parents[i].parentNode.insertBefore(container, parents[i]);
                pseudocode.render(content, container, displayOptions);
                parents[i].style.display = 'none';
                parents[i].parentNode.removeChild(parents[i]);
            }
        });
        </script>
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-68693545-3', 'auto');
          ga('send', 'pageview');
        </script>

    </body>
</html>
